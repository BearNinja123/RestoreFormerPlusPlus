model:
  base_learning_rate: 4.5e-6 
  target: RestoreFormer.models.vqgan_v1.RAModel
  params:
    image_key: 'lq'
    ckpt_path: '/scratch/tnguy231/RF++/experiments/weights/RestoreFormer++.ckpt'
    special_params_lr_scale: 10
    comp_params_lr_scale: 10
    schedule_step: [4000000, 8000000]
    ddconfig:
      target: RestoreFormer.modules.vqvae.vqvae_arch.RANet
      params:
        embed_dim: 256
        n_embed: 1024
        double_z: False
        z_channels: 256
        resolution: 512
        in_channels: 3  
        out_ch: 3
        ch: 64
        ch_mult: [1,2,2,4,4,8]  # num_down = len(ch_mult)-1
        num_res_blocks: 2
        attn_resolutions: [16]
        dropout: 0.0
        enable_mid: True
        fix_decoder: False
        fix_codebook: False
        fix_encoder: False
        head_size: 4
        ex_multi_scale_num: 1

    lossconfig:
      target: RestoreFormer.modules.losses.vqperceptual.VQLPIPSWithDiscriminatorWithCompWithIdentity
      params:
        disc_conditional: False
        disc_in_channels: 3
        #disc_start: 10001
        disc_start: 0
        disc_weight: 1.0
        codebook_weight: 1.0
        identity_weight: 1.5
        use_actnorm: False
        #comp_weight: [1.0, 1.0, 1.0, 0]
        #comp_weight: 1.0
        #comp_style_weight: 0.0
        identity_model_path: /scratch/tnguy231/RF++/experiments/pretrained_models/arcface_resnet18.pth

data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 16 # per device
    num_workers: 7
    train:
      target: RestoreFormer.data.ref_dataset.RefDataset
      params:
        dataroot_gt: /scratch/tnguy231/data/CelebHQRefForRelease/train
        io_backend:
          type: disk
        use_hflip: True
        mean: [0.5, 0.5, 0.5]
        std: [0.5, 0.5, 0.5]
        out_size: 512

        blur_kernel_size: [19,20]
        kernel_list: ['iso', 'aniso']
        kernel_prob: [0.5, 0.5]
        blur_sigma: [0.1, 10]
        downsample_range: [0.8, 8]
        noise_range: [0, 20]
        jpeg_range: [60, 100]

        #color_jitter_prob: ~
        color_jitter_shift: 20
        #color_jitter_pt_prob: ~

        gray_prob: 0.008
        gt_gray: True

        #exposure_prob: ~
        exposure_range: [0.7, 1.1]
        
        shift_prob: 0.2
        shift_unit: 1
        shift_max_num: 32

        uneven_prob: 0.1
        
        hazy_prob: 0.008
        hazy_alpha: [0.75, 0.95]

        #img_size: [512, 512]

        crop_components: False
        component_path: /scratch/tnguy231/RF++/experiments/pretrained_models/FFHQ_eye_mouth_landmarks_512.pth
        eye_enlarge_ratio: 1.4

    validation:
      target: RestoreFormer.data.ref_dataset.RefDataset
      params:
        dataroot_gt: /scratch/tnguy231/data/CelebHQRefForRelease/valid
        io_backend:
          type: disk
        use_hflip: False
        mean: [0.5, 0.5, 0.5]
        std: [0.5, 0.5, 0.5]
        out_size: 512

        blur_kernel_size: [19,20]
        kernel_list: ['iso', 'aniso']
        kernel_prob: [0.5, 0.5]
        blur_sigma: [0.1, 10]
        downsample_range: [0.8, 8]
        noise_range: [0, 20]
        jpeg_range: [60, 100]

        # color jitter and gray
        #color_jitter_prob: ~
        color_jitter_shift: 20
        #color_jitter_pt_prob: ~
        
        #gray_prob: ~
        gt_gray: True

        #exposure_prob: ~
        exposure_range: [0.7, 1.1]
        
        shift_prob: ~
        shift_unit: 1
        shift_max_num: 32

        #uneven_prob: ~
        
        #hazy_prob: ~
        hazy_alpha: [0.75, 0.95]

        #img_size: [512, 512]

        crop_components: False
        component_path: /scratch/tnguy231/RF++/experiments/pretrained_models/FFHQ_eye_mouth_landmarks_512.pth
        eye_enlarge_ratio: 1.4

lightning:
  trainer:
    #devices: "0," # commenting this out defaults to using all GPUs from CUDA_VISIBLE_DEVICES
    accelerator: cuda
    precision: bf16-true
    accumulate_grad_batches: 1
    #max_steps: 256
